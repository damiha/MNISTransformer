{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(text, c_to_i):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for c in text:\n",
    "        res.append(c_to_i[c])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_tokens(tokens, i_to_c):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        res.append(i_to_c[t])\n",
    "        \n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens, context_size):\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx:(idx + self.context_size)]\n",
    "        y = self.tokens[(idx + 1):(idx + self.context_size + 1)]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_and_dicts(filename, context_size):\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    chars = list(sorted(set(text)))\n",
    "    \n",
    "    print(f\"context size = {context_size}\")\n",
    "    print(f\"#symbols = {len(chars)}\")\n",
    "    \n",
    "    c_to_i = {c:i for i, c in enumerate(chars)}\n",
    "    i_to_c = {i:c for i, c in enumerate(chars)}\n",
    "    \n",
    "    tokens = to_tokens(text, c_to_i)\n",
    "    \n",
    "    print(f\"#tokens in text = {len(tokens)}\")\n",
    "    \n",
    "    return TokenDataset(tokens, context_size), c_to_i, i_to_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context size = 32\n",
      "#symbols = 70\n",
      "#tokens in text = 142455\n"
     ]
    }
   ],
   "source": [
    "context_size = 32\n",
    "token_dataset, c_to_i, i_to_c = get_dataset_and_dicts(\"romeo_and_juliet.txt\", context_size=context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(token_dataset, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.device = device\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"n_heads must divide d_model\"\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        # just matrix multiplications\n",
    "        self.key_net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        self.query_net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, _ = x.shape\n",
    "        \n",
    "        # != self.key_net(x).view((B, self.n_heads, T, -1))\n",
    "        # with the above, future leaks into past\n",
    "        keys = self.key_net(x).view((B, T, self.n_heads, -1)).transpose(1, 2)\n",
    "        queries = self.query_net(x).view((B, T, self.n_heads, -1)).transpose(1, 2)\n",
    "        values = self.value_net(x).view((B, T, self.n_heads, -1)).transpose(1, 2)\n",
    "        \n",
    "        scaling_factor = 1.0 / math.sqrt(self.d_model / self.n_heads)\n",
    "        attention_matrices = scaling_factor * torch.matmul(queries, keys.transpose(2, 3))\n",
    "        \n",
    "        neg_inf = -1e10\n",
    "        \n",
    "        # mask the future (upper triangle)\n",
    "        mask = torch.tril(torch.ones(T, T)).to(self.device)\n",
    "        mask = mask.masked_fill(mask == 0, -float(\"inf\"))\n",
    "                        \n",
    "        # softmax per row\n",
    "        activated_attention_matrices = F.softmax(attention_matrices + mask, dim=-1)\n",
    "                \n",
    "        # (B, head, T, dim_per_head)\n",
    "        # d_model = head * dim_per_head\n",
    "        att_output = torch.matmul(activated_attention_matrices, values)\n",
    "        \n",
    "        att_output = torch.transpose(att_output, 1, 2)\n",
    "        \n",
    "        # TODO: add layer norm here\n",
    "        ffn_input = att_output.reshape((B, T, -1)) + x\n",
    "        \n",
    "        ffn_output = self.ffn(ffn_input)\n",
    "        \n",
    "        # TODO: add layer norm here\n",
    "        return ffn_input + ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_symbols, context_length, d_model, n_heads, n_layers, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_symbols = n_symbols\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.device=device\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(num_embeddings=n_symbols, embedding_dim=d_model)\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=context_length, embedding_dim=d_model)\n",
    "        \n",
    "        # TODO: multiple transformer blocks\n",
    "        tbs = [TransformerBlock(d_model = d_model, n_heads = n_heads, device=device) for _ in range(n_layers)]\n",
    "        self.transformer_blocks = nn.Sequential(*tbs)\n",
    "        \n",
    "        self.to_logits = nn.Sequential(\n",
    "            nn.Linear(d_model, n_symbols, device=device)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # batch, time\n",
    "        B, T = x.shape\n",
    "        \n",
    "        embedded = self.token_embedding(x)\n",
    "        #print(f\"{embedded.shape}\")\n",
    "        \n",
    "        positions = torch.arange(T).to(self.device)\n",
    "        #print(f\"{positions.shape}\")\n",
    "\n",
    "        embedded = embedded + self.pos_embedding(positions)\n",
    "        \n",
    "        after_transformer_layers = self.transformer_blocks(embedded)\n",
    "                \n",
    "        return self.to_logits(after_transformer_layers)\n",
    "    \n",
    "    def sample(self, prompt, n_tokens, c_to_i, i_to_c, beta = 1.0):\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "\n",
    "        # Process the prompt to fit within the context length\n",
    "        prompt = prompt[-self.context_length:]\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        \n",
    "        prompt_tokens = [c_to_i[c] for c in prompt]\n",
    "        print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "\n",
    "        context = torch.tensor(prompt_tokens, dtype=torch.long).unsqueeze(0).to(device)  # Ensure context is 2D: 1 x sequence_length\n",
    "\n",
    "        output = []\n",
    "        for _ in range(n_tokens):\n",
    "            with torch.no_grad():\n",
    "                logits = self(context)[:, -1, :] / beta  # Get logits for the last token position only\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                last_sampled_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                output.append(last_sampled_token.item())\n",
    "                context = torch.cat((context, last_sampled_token), dim=1)[:, -self.context_length:]  # Update context\n",
    "                \n",
    "                \n",
    "        response = ''.join([i_to_c[t] for t in output])\n",
    "        print(f\"Response: {response}\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_symbols = len(c_to_i.keys())\n",
    "\n",
    "transformer = Transformer(n_symbols, context_size, d_model = 512, n_heads = 8, n_layers=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(transformer, train_loader, val_loader, n_epochs,\n",
    "          optimizer=None,\n",
    "          lr_scheduler=None,\n",
    "          early_stopper=None, \n",
    "         ):\n",
    "    \n",
    "    transformer = transformer.to(device)\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(transformer.parameters(), lr=3e-4)\n",
    "        print(\"Using default optimizer\")\n",
    "        \n",
    "    if early_stopper is None:\n",
    "        early_stopper = EarlyStopper(patience=3, min_delta=1e-2)\n",
    "        print(\"Using default early stopper\")\n",
    "        \n",
    "    if lr_scheduler is None:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                                  factor=0.3, patience=3, min_lr=1e-5,\n",
    "                                                                  threshold=1e-3\n",
    "                                                                 )\n",
    "        print(\"Using default LR scheduler\")\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses_over_epochs = []\n",
    "    val_losses_over_epochs = []\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        train_losses_this_batch = []\n",
    "        transformer.train()\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            \n",
    "            # to GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            logits = transformer(batch_x)\n",
    "            \n",
    "            logits = logits.transpose(1, 2)\n",
    "            \n",
    "            loss = criterion(logits, batch_y)\n",
    "            \n",
    "            train_losses_this_batch.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss_this_epoch = np.mean(np.array(train_losses_this_batch))\n",
    "        train_losses_over_epochs.append(train_loss_this_epoch)\n",
    "        \n",
    "        # for early stopping\n",
    "        val_losses_this_batch = []\n",
    "        \n",
    "        transformer.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(val_loader):\n",
    "\n",
    "                # to GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                logits = transformer(batch_x)\n",
    "                \n",
    "                logits = logits.transpose(1, 2)\n",
    "\n",
    "                loss = criterion(logits, batch_y)\n",
    "\n",
    "                val_losses_this_batch.append(loss.item())\n",
    "        \n",
    "        val_loss_this_epoch = np.mean(np.array(val_losses_this_batch))\n",
    "        val_losses_over_epochs.append(val_loss_this_epoch)\n",
    "        print(f\"{epoch_idx}. avg. train loss = {train_loss_this_epoch}, avg. val loss = {val_loss_this_epoch}\")\n",
    "        \n",
    "        should_stop = early_stopper.early_stop(val_loss_this_epoch)\n",
    "        lr_scheduler.step(val_loss_this_epoch)\n",
    "        \n",
    "        if should_stop:\n",
    "            print(f\"stopping early (val. loss did not decrease for {early_stopper.patience})\")\n",
    "            break\n",
    "        \n",
    "    return train_losses_over_epochs, val_losses_over_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default early stopper\n",
      "Using default LR scheduler\n",
      "0. avg. train loss = 0.6658949276681165, avg. val loss = 0.6295139496879918\n",
      "1. avg. train loss = 0.5508448999337475, avg. val loss = 0.5450061815125602\n",
      "2. avg. train loss = 0.48529759942481815, avg. val loss = 0.498303263847317\n",
      "3. avg. train loss = 0.44516120916263824, avg. val loss = 0.46687306969293524\n",
      "4. avg. train loss = 0.4209693576617942, avg. val loss = 0.4513879711074488\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(transformer, train_loader, val_loader, n_epochs=5, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You are\n",
      "Prompt tokens: [33, 52, 58, 1, 38, 55, 42]\n",
      "Response:  a joyful bride.\n",
      "I wonder at this haste, that I must wed\n",
      "Ere he that shot so trim\n",
      "Whinishe heaven bless thee. Hark you, sir.\n",
      "\n",
      "ROMEO.\n",
      "What wilt thou tell her, Nurse? Thou dost not mark while. God Benvolio and Romeo.\n",
      "\n",
      "BENVOLIO.\n",
      "Here comes the furious Tybalt back again.\n",
      "\n",
      " [_Exit._]\n",
      "\n",
      "ROMEO.\n",
      "[_To Juliet._] If I profane with my unworthiest hand\n",
      "This toolboys requestainers of this neighbour-stained steel,—\n",
      "We’ll warrant thee, Nurse. Commend me to thy lady and my wife!\n",
      "\n",
      "JULIET.\n",
      "That same I am done. For thou hast sold one what he best friend I how not?\n",
      "\n",
      "JULIET.\n",
      "My ear new in my house of Montague’s.\n",
      "\n",
      "PA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' a joyful bride.\\nI wonder at this haste, that I must wed\\nEre he that shot so trim\\nWhinishe heaven bless thee. Hark you, sir.\\n\\nROMEO.\\nWhat wilt thou tell her, Nurse? Thou dost not mark while. God Benvolio and Romeo.\\n\\nBENVOLIO.\\nHere comes the furious Tybalt back again.\\n\\n [_Exit._]\\n\\nROMEO.\\n[_To Juliet._] If I profane with my unworthiest hand\\nThis toolboys requestainers of this neighbour-stained steel,—\\nWe’ll warrant thee, Nurse. Commend me to thy lady and my wife!\\n\\nJULIET.\\nThat same I am done. For thou hast sold one what he best friend I how not?\\n\\nJULIET.\\nMy ear new in my house of Montague’s.\\n\\nPA'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.sample(prompt=\"You are\", n_tokens = 600, c_to_i=c_to_i, i_to_c=i_to_c, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inpainting",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
